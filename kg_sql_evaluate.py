import requests
import json
import re



def evaluate_score(model='deepseek-r1:32b', host='http://localhost:11434', user_query='', answer='', ground_truth=''):


    # prompt = f'''You are a helpful agent who is good at evaluating the answer of a question compared with the ground truth.
    # Now, you will receive a [User query], an [Answer] and a [Ground Truth]. 
    # The [User query] are questions focused on a product with some specific features, asking about the price, availability, or other details.
    # The [Answer] is a natural language response generated by an agent, which may contain some information about the product and the target the user ask about.
    # The [Ground Truth] is the correct answer to the question, which is in json format. It usually contains the product name, price, weight and other details.

    # Your task is to evaluate the [Answer] based on the [User query] and [Ground Truth].You should check if the [Answer] contains the correct information in the [Ground Truth], such as the product name, price, and other details.
    # If the [Answer] perfectly matches the [Ground Truth], the final score should be 100. Note, a perfect match means the [Answer] contains all the information in the [Ground Truth], and no extra information and no miss information.
    # If the [Answer] do not perfectly match the [Ground Truth], but miss some information in the [Ground truth], or contains extra information that is not in the [Ground Truth], You should give a reasonable score. For example, if [Ground Truth] contains two key inforamtion, but the [Answer] only contains one matching information, the score can be 50.
    # If the [Answer] does not contain any information in the [Ground Truth], the final score should be 0.
    # When giving the score, you should ONLY condiser the matching rate of the product and target features of the [Ground Truth], do not consider the tone, style, or other aspects of the [Answer].
    # '''
    prompt='''
    You are a helpful agent who is good at evaluating answers against ground truth data.
    You will receive:
    - [User query]: A question about product features (price, availability, etc.)
    - [Answer]: Agent's natural language response
    - [Ground Truth]: Correct answer in JSON format (array of products with attributes)

    Evaluation Rules:
    1. **Perfect Match (100)**: Answer must:
    - Mention every product in Ground Truth
    - Include all attributes from Ground Truth for each product
    - Contain NO extra products/attributes
    - Have NO incorrect information

    2. **Partial Match (Adjusted Score)**:
    - Score = (Number of correctly reported products / Total products in GT) × 100
    - A "correctly reported product" requires:
        * Product name matches GT exactly
        * All GT attributes present and correct
        * No extra attributes for that product
    - Round to nearest whole number

    3. **Zero Score (0)**:
    - No products from GT mentioned
    - All mentioned products are incorrect

    4. **Constraints**:
    - Ignore formatting/tone/style
    - Treat attributes as case-insensitive (e.g., "$399" = 399)
    - Ignore informational additions (e.g., feature explanations) unless they contradict GT
        '''

    full_prompt = (f'''{prompt} \n\n'''
                f''' Now, do this for [User query]: {user_query}, [Answer]: {answer}, [Ground Truth]: {ground_truth}''' 
                f''' Please just return the final score, do not return any other information.''' )

    url = f'{host}/api/generate'
    payload = {
        "model": model,
        "prompt": full_prompt,
        "stream": False
    }

    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        return response.json().get("response", "")
    except Exception as e:
        return f"❌ 出错：{e}"

with open ('/home/NingyuanXiao/Vanna_test/select_results_truth.json', 'r') as f:
    data = json.load(f)

for item in data:
    user_query = item.get("Question", "")
    answer = item.get("Summary Result", "")
    ground_truth = item.get("answer", "")

    if not ground_truth:
        print(f"❌ No ground truth for query: {user_query}")
        continue

    score = evaluate_score(user_query=user_query, answer=answer, ground_truth=ground_truth)
    score = re.sub(r'<think>.*?</think>', '', score, flags=re.DOTALL).strip()
    print(f"User Query: {user_query}\nScore: {score}\n")
    print("===" * 20)
